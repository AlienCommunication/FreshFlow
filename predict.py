# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yTngxacd2CWyTidGu1psVdxq5eb2zMcN
"""

import pandas as pd
import numpy as np
import seaborn as sns
import warnings
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import f_oneway
from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic, ccf
import statsmodels.api as sm
from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from statsmodels.tsa.stattools import acf, pacf
from pandas import Series
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from matplotlib import pyplot
from pandas import Series
from matplotlib import pyplot
from statsmodels.tsa

df=pd.read_csv("/content/drive/MyDrive/My Grih Laxmi/freshflow-main/freshflow/data.csv")
df.drop("Unnamed: 0",axis=1,inplace=True)
df["day"]=pd.to_datetime(df["day"])
df.set_index("day",inplace=True)
df.head(3)

def eda(df):
    print("Statistical description of data is: ")
    print(df.describe())
    print()
    print("Null values are :")
    print(df.isnull().sum())
    print()
    print("Treating null values by filliling it with mean values")
    df.fillna(df.mean(),inplace=True)
    print()
    print("Null values after treatment are : ")
    print(df.isnull().sum())
    print()
    print("correlation between variables is: ")
    sns.heatmap(df.corr(),linewidth=0.2,vmax=1.0,square=True,linecolor='red',annot=True)
eda(df)



num_df=df[['item_number', 'purchase_price', 'suggested_retail_price',
       'orders_quantity', 'sales_quantity', 'revenue']]

"""Plotting data of numerical columns"""

for col in num_df.columns:
    plt.figure(figsize=(15,3))
    plt.plot(num_df[col])
    plt.show()

"""Distributions plot using kde and histogram"""

def visualization(num_df):
    def distribution_check(num_df):
        pyplot.figure(1)
        pyplot.subplot(211)
        num_df.hist()
        pyplot.subplot(212)
        print()
        print()
        print()
        num_df.plot(kind='kde')
        pyplot.show()

    # using ACF and PACF plots:
    def stationarity_acf_pacf(num_df):
        pyplot.figure(figsize=(10,6))
        pyplot.subplot(211)
        plot_acf(num_df, ax=pyplot.gca(),lags=12)
        pyplot.subplot(212)
        plot_pacf(num_df, ax=pyplot.gca(),lags=12)
        print()
        print()
        print()
        pyplot.show()

    print("Distribution of various features")
    for col in num_df:
        distribution_check(num_df[col])

    print("Box plot for outlier detection")
    # box whisker plot
    for col in num_df:
        fig, ax = plt.subplots(figsize=(15,3))
        sns.boxplot(num_df[col].index.year, num_df[col], ax=ax)

    print("checking stationarity using acf and pacf plots")
    for col in num_df:
        stationarity_acf_pacf(num_df[col])

    sns.barplot(x = 'item_name',
            y = 'sales_quantity',
            data=df ) 
 
    # Show the plot
    plt.show()

visualization(num_df)

"""Decomposing using statsmodel:

We can use statsmodels to perform a deccomposition IT deconstructs a time series into several components, each representing one of the underlying categories of patterns. With statsmodels we will be able to see the trend, seasonal, and residual components of our data.
"""

def decomposition(df):
    decomposition = sts.seasonal_decompose(df,model='additive', freq=12)

    trend = decomposition.trend
    seasonal = decomposition.seasonal
    residual = decomposition.resid

    plt.subplot(411)
    df.plot(kind="line",figsize=(10,6),label='Original')
    plt.subplot(412)
    trend.plot(kind="line",figsize=(10,6),label='trend')
    plt.legend(loc='best')
    plt.subplot(413)
    seasonal.plot(kind="kde",figsize=(10,6),label='Seasonality')
    plt.legend(loc='best')
    plt.subplot(414)
    residual.plot(kind="line",figsize=(10,6),label='Residuals')
    plt.legend(loc='best')
    plt.tight_layout()

decomposition(num_df["revenue"])

decomposition(num_df["orders_quantity"])

"""Checking stationarity using dickey fuller test"""

def test_stationarity(num_df):
    #Determing rolling statistics
    #rolmean = pd.rolling_mean(timeseries, window=12)
    #rolstd = pd.rolling_std(timeseries, window=12)
    rolmean= pd.core.window.Rolling(num_df, window=12).mean()
    rolstd = pd.core.window.Rolling(num_df, window=12).std()
    #rolmean = timeseries.rolling(12).mean()
    #rolstd  = timeseries.rolling(12).std()
    
    #Plot rolling statistics:
    orig = plt.plot(num_df, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)
    
    #Perform Dickey-Fuller test:
    print ('Results of Dickey-Fuller Test:')
    dftest = adfuller(num_df, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print (dfoutput)
    
# H0: series is nonstationary
# H1: series is stationary

for col in num_df:
    test_stationarity(num_df[col])

"""Feature Engineering"""

import datetime as dt
num_df=num_df.reset_index()
num_df['profit'] = num_df['suggested_retail_price'] - num_df['purchase_price']
num_df['week_day'] = num_df["day"].dt.dayofweek
num_df['month_day'] = num_df['day'].dt.day
num_df['month'] = num_df['day'].dt.month
num_df.set_index("day",inplace=True)
num_df.head(3)

"""Train Test"""

#Training and test set
test_days = 30
training_set = num_df.iloc[:-test_days, :]
test_set = num_df.iloc[-test_days:, :]
test_set.tail(1)

training_set.head()

test_set.head(2)

training_set.columns

#exogenous variables
train_exog = training_set[['item_number', 'purchase_price', 'suggested_retail_price',
       'orders_quantity', 'revenue', 'profit', 'week_day',
       'month_day', 'month']]

test_exog = test_set[['item_number', 'purchase_price', 'suggested_retail_price',
       'orders_quantity', 'revenue', 'profit', 'week_day',
       'month_day', 'month']]
       
test_exog.head()

pip install pmdarima

"""# LOADING AUTO ARIMA MODEL"""

# load the model from disk
import pickle
filename="/content/drive/MyDrive/assignment_freshflow/models/AutoArima_model.sav"
loaded_model_auto_arima = pickle.load(open(filename, 'rb'))

#predictions
predictions_sarimax = pd.Series(loaded_model_auto_arima.predict(n_periods= test_days,
                              X = test_exog)).rename("SARIMAX")
predictions_sarimax.index = test_set.index                              
predictions_sarimax

"""Model Assessment"""

#MAE and RMSE
from sklearn.metrics import mean_squared_error, mean_absolute_error
print(round(mean_absolute_error(test_set['sales_quantity'], predictions_sarimax),0))
print(round(np.sqrt(mean_squared_error(test_set['sales_quantity'], predictions_sarimax)), 0))

"""# LOADING TBATS MODEL"""

# load the model from disk
import pickle
filename="/content/drive/MyDrive/assignment_freshflow/models/TBATS.sav"
loaded_model_tbat = pickle.load(open(filename, 'rb'))

#predictions
predictions = pd.Series(loaded_model_tbat.forecast(steps = test_days)).rename("TBATS")
predictions.index = test_set.index

predictions

from sklearn.metrics import mean_squared_error
np.sqrt(mean_squared_error(test_set.sales_quantity,
                           predictions))

